---
title: 'Scrapyå…¥é—¨'
date: 2017-04-08 10:39:38
tags: ['python','spider',Scrapy]
category: ['ç¬”è®°','Scrapy']
---
åšä¸ªçˆ¬è™«è¯•è¯•å§ï¼Œæ‰“å‘æ— èŠæ—¶å…‰

è¿™é‡Œçœ‹åˆ°äº†yieldå…³é”®å­—,pythonçš„ä¸œè¥¿è¿˜æ˜¯ä¸ç†Ÿ,ç™¾åº¦äº†ä¸€ä¸‹å¤§æ¦‚æœ‰ä¸ªæ¦‚å¿µ,æ…¢æ…¢ç ”ç©¶,å…ˆæ”¾ä¸ª[é“¾æ¥][yield]markä¸€ä¸‹

[yield]: http://www.jianshu.com/p/d09778f4e055 "å½»åº•ç†è§£Pythonä¸­çš„yield"

<!--more-->

# æœ€ç®€å•çš„çˆ¬ç½‘é¡µ

å…ˆè´´ä¸€æ®µä»£ç ï¼Œç®€å•çš„æ²¡ä»€ä¹ˆå¥½è®²

```py
#!/usr/bin/python2.7
# -*- coding: utf-8 -*-
import urllib2
def getHtml(url):
    page = urllib2.urlopen(url)
    html=page.read()
    return html
url="https://hisashiburidane.github.io"
html=getHtml(url)
print(html)
```

# Scrapy

ç™¾åº¦äº†ä¸€ä¸‹æ¥ä¸‹æ¥åº”è¯¥æ€ä¹ˆåšï¼Œæ— éå…ˆæ‹‰æ•°æ®ï¼Œç„¶åä½¿ç”¨æ­£åˆ™æˆ–è€…å…¶ä»–æ‰‹æ®µæŠŠç›®æ ‡æ•°æ®æå–å‡ºæ¥ï¼Œä¹‹åè¦èƒ½è‡ªåŠ¨ç¿»é¡µä¹‹ç±»çš„ï¼Œèƒ½å¤ŸæŒç»­æ‹‰
è¯´èµ·æ¥ç®€å•ï¼Œè·å–ç½‘é¡µå†…å®¹åªéœ€è¦ç¬¬ä¸€æ­¥é‚£å‡ è¡Œä»£ç ï¼Œç„¶åå°±è¦ä»”ç»†ç¢ç£¨ä¸€ä¸‹æ€ä¹ˆåšäº†ï¼Œç™¾åº¦çš„ç»“æœæ˜¯ç”¨[Scrapy][Scrapyå®˜ç½‘]è¿™ä¸ªæ¡†æ¶çš„æ¯”è¾ƒå¤šï¼Œå…ˆç®€å•å­¦ä¹ ä¸€ä¸‹å§
![input](scrapy_architecture_02.png)
çœ‹å®Œæ–‡æ¡£åˆè¸©äº†å¥½å¤šå‘â€¦â€¦ç°åœ¨å¼€å§‹è¡¥å……
é¡ºç€æ–‡æ¡£çœ‹ä¸‹æ¥ï¼Œ`pip install`æŒ‰è¯´åº”è¯¥å¾ˆç®€å•å•Šï¼Œå¯æ˜¯OSXä¸‹é¢å®‰è£…å°±æ˜¯å‡ºäº†ä¸€å †é—®é¢˜ï¼Œæ¯”å¦‚ç³»ç»Ÿæƒé™äº†ï¼Œè¿˜æœ‰ä¸€ä¸ªä»€ä¹ˆç³»ç»Ÿä¿æŠ¤æœºåˆ¶äº†ï¼Œä¸èƒ½åœ¨usrå’Œetcè¿™ç§ç›®å½•ä¸‹è£…ä¸œè¥¿â€¦â€¦çœ‹äº†ä¸€åœˆï¼Œä¸€ç§è§£å†³åŠæ³•æ˜¯é…ç½®pipå®‰è£…è·¯å¾„ï¼Œè£…åœ¨å½“å‰ç”¨æˆ·çš„ç›®å½•ä¸‹ï¼Œè¦æ”¹ä¸€å †é…ç½®æ–‡ä»¶
æœ€åå†³å®šè¦é‡‡ç”¨çš„æ˜¯[virtualenv][visualenvå®˜ç½‘]ï¼Œå› ä¸ºä¸ç”¨è¿™ä¹ˆéº»çƒ¦â€¦â€¦
å†™åœ¨ä¸€èµ·å¤ªä¹±äº†ï¼Œæ”¾ä¸ªé“¾æ¥{% post_link pythonå¼€å‘ç¯å¢ƒçš„ä¸€äº›é…ç½® pythonå¼€å‘ç¯å¢ƒçš„ä¸€äº›é…ç½® %}

## Scrapyå®‰è£…

æå®šäº†è™šæ‹Ÿç¯å¢ƒï¼Œå°±å¯ä»¥å¼€å§‹å®‰è£…é…ç½®Scrapyäº†

```sh
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒç›®å½•(?æš‚æ—¶è¿™æ ·å«å§)
$ visualenv ENV
# åˆ‡æ¢è‡³è™šæ‹Ÿç¯å¢ƒ
$ source ENV/bin/activate
# é€€å‡ºè™šæ‹Ÿç¯å¢ƒ
$ deactivate
# pipå®‰è£…
$ pip install Scrapy
# åˆ›å»ºåä¸ºtutorialçš„scrapyé¡¹ç›®
$ scrapy startproject tutorial
```

## Scrapy Tuurtorial

ä¸‹é¢æ˜¯[Scrapyå®˜ç½‘Tutorial][Scrapyå®˜ç½‘Tutorial]ç»™å‡ºçš„æ•™ç¨‹ä¸­çš„ä»£ç 

### æŠ“å–ç½‘é¡µ

```py
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
```

ä¹‹ååœ¨tutorialç›®å½•ä¸‹æ‰§è¡Œå‘½ä»¤

```sh
$ scrapy crawl quotes
# This command runs the spider with name quotes that weâ€™ve just added, that will send some requests for the quotes.toscrape.com domain. You will get an output similar to this:
... (omitted for brevity)
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened
2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)
...
```

>Scrapy schedules the scrapy.Request objects returned by the start_requests method of the Spider. Upon receiving a response for each one, it instantiates Response objects and calls the callback method associated with the request (in this case, the parse method) passing the response as argument.

è¿™æ—¶å€™ENVç›®å½•ä¸‹ä¿å­˜äº†ä¸¤ä¸ªhtmlæ–‡ä»¶`quotes-1.html`,`quotes-2.html`,è¿™å°±æ˜¯çˆ¬ä¸‹æ¥çš„ä¸œè¥¿äº†

æ¥ä¸‹æ¥æ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç°

```py
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes_shortcut"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self,response):
        page = response.url.split("/")[-2]
        finename = 'quotes_shortcut-%s.html' % page
        with open(finename,'wb') as f:
            f.write(response.body)
```

>The parse() method will be called to handle each of the requests for those URLs, even though we havenâ€™t explicitly told Scrapy to do so. This happens because parse() is Scrapyâ€™s default callback method, which is called for requests without an explicitly assigned callback.

çœ‹äº†ä¸€ä¸‹,è¿™ä¸€æ®µå¤§æ¦‚æ˜¯ä¸ºäº†è¯´æ˜parseæ˜¯scrapyçš„é»˜è®¤å›è°ƒå‡½æ•°,ä¸éœ€è¦ä¸‹é¢è¿™ä¸€æ®µä»£ç æŒ‡å®šä¹Ÿä¼šè¢«é»˜è®¤è°ƒç”¨

```py
for url in urls:
    yield scrapy.Request(url=url, callback=self.parse)
```

### CSSSelectoræå–æ•°æ®

ç„¶åæ˜¯è®²scrapyå¦‚ä½•æå–æ•°æ®çš„

#### .extract() method

```sh
# scrapy shell
$ scrapy shell 'http://quotes.toscrape.com/page/1/'
# æ‰§è¡Œå®Œæ¯•ä¹‹åçœ‹åˆ°å¦‚ä¸‹è¾“å‡º
[ ... Scrapy log here ... ]
2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x7fa91d888c90>
[s]   item       {}
[s]   request    <GET http://quotes.toscrape.com/page/1/>
[s]   response   <200 http://quotes.toscrape.com/page/1/>
[s]   settings   <scrapy.settings.Settings object at 0x7fa91d888c10>
[s]   spider     <DefaultSpider 'default' at 0x7fa91c8af990>
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser
>>>
# æ¥ä¸‹æ¥å¯ä»¥åœ¨scrapy shellä¸­è¿›è¡Œä¸€äº›å‘½ä»¤æ“ä½œ
# css selector
>>> response.css('tittle')
[<Selector xpath=u'descendant-or-self::title' data=u'<title>Quotes to Scrape</title>'>]
>>> response.css('tittle::text')
[<Selector xpath=u'descendant-or-self::title/text()' data=u'Quotes to Scrape'>]
>>> response.css('title::text').extract()
[u'Quotes to Scrape']
>>> response.css('title').extract()
[u'<title>Quotes to Scrape</title>']
# extract()æ–¹æ³•å¤„ç†çš„æ˜¯ä¸€ä¸ªselectorListå®ä¾‹,å¦‚æœåªæƒ³è¦ç¬¬ä¸€ä¸ªç»“æœå¯ä»¥å¦‚ä¸‹æ“ä½œ
>>> response.css('title::text').extract_first()
u'Quotes to Scrape'
# ä¹Ÿå¯ä»¥åƒæ•°ç»„ä¸€æ ·å†™
# ä½¿ç”¨.extract_first()æ–¹æ³•åœ¨æ•°ç»„é•¿åº¦ä¸º0(æ²¡æœ‰æ‰¾åˆ°å¯¹åº”é€‰æ‹©å™¨çš„å…ƒç´ )çš„æ—¶å€™å¯ä»¥è¿”å›Noneè€Œä¸æ˜¯å¼•å‘ç´¢å¼•é”™è¯¯
# However, using .extract_first() avoids an IndexError and returns None when it doesnâ€™t find any element matching the selection.
>>> response.css('title::text')[0].extract()
u'Quotes to Scrape'
```

#### regular expressions

>Besides the extract() and extract_first() methods, you can also use the re() method to extract using regular expressions:

æ­£åˆ™è¡¨è¾¾å¼æå–æ•°æ®

```sh
>>> response.css('title::text').re(r'Quotes.*')
[u'Quotes to Scrape']
>>> response.css('title::text').re(r'Q\w+')
[u'Quotes']
>>> response.css('title::text').re(r'(\w+) to (\w+)')
[u'Quotes', 'Scrape']
```

ä¸ºäº†éªŒè¯æå–åˆ°çš„æ•°æ®æ˜¯å¦æ­£ç¡®,å¯ä»¥è°ƒç”¨`view(response)`æ–¹æ³•æ‰“å¼€æµè§ˆå™¨,åœ¨å¼€å‘è€…å·¥å…·ä¸­æŸ¥çœ‹é¡µé¢å…ƒç´ 

### XPathæå–æ•°æ®

è¿™ä¸€å—ä¹‹å‰åœ¨åšseleniumçš„ç«¯å¯¹ç«¯æµ‹è¯•çš„æ—¶å€™æœ‰ä¸€ç‚¹ç‚¹äº†è§£,å¯ä»¥ç”¨çš„æ—¶å€™å†ç™¾åº¦

```sh
>>> response.xpath('//title')
[<Selector xpath='//title' data=u'<title>Quotes to Scrape</title>'>]
>>> response.xpath('//title/text()').extract_first()
u'Quotes to Scrape'
```

>XPath expressions are very powerful, and are the foundation of Scrapy Selectors. In fact, CSS selectors are converted to XPath under-the-hood. You can see that if you read closely the text representation of the selector objects in the shell.

è¿™é‡Œè¯´XPathæ˜¯CSSSelectorçš„åŸºç¡€,æ„æ€æ˜¯æ›´å¼ºå¤§äº†

### è¿™é‡Œè®²å¦‚ä½•æå–æœ€ç»ˆçš„ç›®æ ‡æ•°æ®

è¿™ç¯‡æ•™ç¨‹ä¸­çš„ç›®çš„æ˜¯ä»ç½‘ç«™ä¸­æŠ“å–ä½œå“å’Œä½œè€…æ•°æ®,æˆ‘çŒœæ˜¯ç”¨æ­£åˆ™ğŸ™„
æŠ“åˆ°çš„ç½‘ç«™æ•°æ®æ ¼å¼å¦‚ä¸‹

```html
<div class="quote">
    <span class="text">â€œThe world as we have created it is a process of our
    thinking. It cannot be changed without changing our thinking.â€</span>
    <span>
        by <small class="author">Albert Einstein</small>
        <a href="/author/Albert-Einstein">(about)</a>
    </span>
    <div class="tags">
        Tags:
        <a class="tag" href="/tag/change/page/1/">change</a>
        <a class="tag" href="/tag/deep-thoughts/page/1/">deep-thoughts</a>
        <a class="tag" href="/tag/thinking/page/1/">thinking</a>
        <a class="tag" href="/tag/world/page/1/">world</a>
    </div>
</div>
```

è¿™é‡Œé‡æ–°æ‰“å¼€scrapy shell

```sh
$ scrapy shell 'http://quotes.toscrape.com'
# ä½¿ç”¨CSSSelectorè·å–åˆ°äº†æ‰€æœ‰çš„div.quoteå…ƒç´ å¯¹è±¡
>>> response.css("div.quote")
# quoteç°åœ¨å°±æ˜¯ç¬¬ä¸€ä¸ªå…ƒç´ å¯¹è±¡
>>> quote=response.css("div.quote")[0]
>>> quote
<Selector xpath=u"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]" data=u'<div class="quote" itemscope itemtype="h'>
# ç„¶åæ ¹æ®quoteçš„å…ƒç´ å»æŸ¥æ‰¾ä¸‹é¢çš„å…ƒç´ 
>>> title = quote.css("span.text::text").extract_first()
>>> title
u'\u201cThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.\u201d'
>>> author = quote.css("small.author::text").extract_first()
>>> author
'Albert Einstein'
>>> tags = quote.css("div.tags a.tag::text").extract()
>>> tags
[u'change', u'deep-thoughts', u'thinking', u'world']
```

```sh
>>> for quote in response.css("div.quote"):
...     text=quote.css("span.text::text").extract_first()
...     author=quote.css("small.author::text").extract_first()
...     tags=quote.css("div.tags a.tag::text").extract()
...     print(dict(text=text,author=author,tags=tags))
...
{'text': u'\u201cThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.\u201d', 'tags': [u'change', u'deep-thoughts', u'thinking', u'world'], 'author': u'Albert Einstein'}
{'text': u'\u201cIt is our choices, Harry, that show what we truly are, far more than our abilities.\u201d', 'tags': [u'abilities', u'choices'], 'author': u'J.K. Rowling'}
... a few more of these, omitted for brevity
```

### åœ¨Spiderä¸­æå–æ•°æ®

```py
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes_extract"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
        'http://quotes.toscrape.com/page/2/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('small.author::text').extract_first(),
                'tags': quote.css('div.tags a.tag::text').extract(),
            }
```

è¿è¡Œç¨‹åºçœ‹è¾“å‡º,å¤§æ¦‚å°±æ˜¯ä¸‹é¢è¿™æ ·

```sh
$ scrapy crawl quotes_extract
...
2017-04-08 22:00:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
{'text': u'\u201cThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.\u201d', 'tags': [u'change', u'deep-thoughts', u'thinking', u'world'], 'author': u'Albert Einstein'}
2017-04-08 22:00:06 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>
{'text': u'\u201cIt is our choices, Harry, that show what we truly are, far more than our abilities.\u201d', 'tags': [u'abilities', u'choices'], 'author': u'J.K. Rowling'}
...
# è¾“å‡ºjsonæ–‡ä»¶
$ scrapy crawl quotes_extract -o quotes_extract.json
# è¾“å‡ºjlæ–‡ä»¶
$ scrapy crawl quotes_extract -o quotes_extract.jl
```

>The JSON Lines format is useful because itâ€™s stream-like, you can easily append new records to it. It doesnâ€™t have the same problem of JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like JQ to help doing that at the command-line.

è¿™é‡Œç»ˆç«¯å’Œä¸Šé¢çš„è¾“å‡ºæ˜¯ä¸€æ ·çš„,åªæ˜¯æŠŠæ•°æ®ä¿å­˜åœ¨äº†jsonæ–‡ä»¶é‡Œ,éšä¾¿åˆ—å‡ æ¡çœ‹çœ‹æ•ˆæœå°±è¡Œäº†

```json
[
{"text": "\u201cThe world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.\u201d", "author": "Albert Einstein", "tags": ["change", "deep-thoughts", "thinking", "world"]},
{"text": "\u201cIt is our choices, Harry, that show what we truly are, far more than our abilities.\u201d", "author": "J.K. Rowling", "tags": ["abilities", "choices"]},
{"text": "\u201cThere are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.\u201d", "author": "Albert Einstein", "tags": ["inspirational", "life", "live", "miracle", "miracles"]},
{"text": "\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d", "author": "Jane Austen", "tags": ["aliteracy", "books", "classic", "humor"]},
{"text": "\u201cImperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.\u201d", "author": "Marilyn Monroe", "tags": ["be-yourself", "inspirational"]},
{"text": "\u201cTry not to become a man of success. Rather become a man of value.\u201d", "author": "Albert Einstein", "tags": ["adulthood", "success", "value"]},
...
]
```

>In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an Item Pipeline. A placeholder file for Item Pipelines has been set up for you when the project is created, in tutorial/pipelines.py. Though you donâ€™t need to implement any item pipelines if you just want to store the scraped items.

è¿™ä¸ªæ˜¯ç®¡é“äº†,è¿™éƒ¨åˆ†å›å¤´å†çœ‹å§,ä»Šå¤©å…ˆåˆ°è¿™é‡Œ.ä¹ˆæœ‰æ•ˆç‡...

### å¦‚ä½•æŒç»­æŸ¥æ‰¾æ‰€æœ‰ç½‘é¡µ

è¿™é‡Œåº”è¯¥å°±æ˜¯æ‰¾åˆ°"ä¸‹ä¸€é¡µ"çš„æ ‡è®°,ç„¶åæ— é™å‘ä¸‹ç›´åˆ°æ²¡æœ‰ä¸‹ä¸€é¡µä¸ºæ­¢,ä¹Ÿæ²¡ä»€ä¹ˆéš¾åº¦,æŒ‡å®šé€‰æ‹©å™¨è€Œå·².
ä¸‹é¢æ˜¯quotesç½‘ç«™çš„ä¸‹ä¸€é¡µå…ƒç´ 

```html
<ul class="pager">
    <li class="next">
        <a href="/page/2/">Next <span aria-hidden="true">&rarr;</span></a>
    </li>
</ul>
```

shellé‡Œé¢å…ˆçœ‹çœ‹

```sh
$ scrapy shell 'http://quotes.toscrape.com/'
>>> response.css('li.next a::attr(href)').extract_first()
u'/page/2/'
```

```py
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes_next"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('small.author::text').extract_first(),
                'tags': quote.css('div.tags a.tag::text').extract(),
            }

        next_page = response.css('li.next a::attr(href)').extract_first()
        if next_page is not None:
            # ä»ç›¸å¯¹è·¯å¾„è·å–é¡µé¢çš„ç»å¯¹è·¯å¾„
            # http://quotes.toscrape.com/page/1/
            # /page/2/
            # æ‹¼èµ·æ¥å°±æ˜¯http://quotes.toscrape.com/page/2/è¿™æ ·
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)
```

>What you see here is Scrapyâ€™s mechanism of following links: when you yield a Request in a callback method, Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes.

scrapyå›è°ƒparseçš„æ—¶å€™ä¼šè§¦å‘next_pageä¸‹é¢çš„`scrapy.Request(next_page, callback=self.parse)`ä»£ç ,å¾ªç¯ç›´åˆ°`next_page=None`

```py
import scrapy


class AuthorSpider(scrapy.Spider):
    name = 'quotes_author'

    start_urls = ['http://quotes.toscrape.com/']

    def parse(self, response):
        # follow links to author pages
        for href in response.css('.author + a::attr(href)').extract():
            yield scrapy.Request(response.urljoin(href),
                                 callback=self.parse_author)

        # follow pagination links
        next_page = response.css('li.next a::attr(href)').extract_first()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)

    def parse_author(self, response):
        def extract_with_css(query):
            return response.css(query).extract_first().strip()

        yield {
            'name': extract_with_css('h3.author-title::text'),
            'birthdate': extract_with_css('.author-born-date::text'),
            'bio': extract_with_css('.author-description::text'),
        }
```

è¿™ä¸€æ®µæ˜¯å…ˆåŠ è½½é¡µé¢ä¹‹åæ‹¿åˆ°æ‰€æœ‰çš„ä½œè€…çš„é“¾æ¥,ç„¶ååœ¨parseä¸­æ–°å»ºscrapy.Request()è¯·æ±‚,è§¦å‘parse_authorå›è°ƒ,åœ¨ä½œè€…é¡µé¢ä¸­æ‹¿åˆ°ä½œè€…ä¿¡æ¯
æˆ‘åœ¨æƒ³è¿™ä¸ªä¸œè¥¿ä¼šä¸ä¼šä¸€ä¸å°å¿ƒææˆé€’å½’äº†...
>Another interesting thing this spider demonstrates is that, even if there are many quotes from the same author, we donâ€™t need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured by the setting DUPEFILTER_CLASS.

è¯è¯´è¿™æ˜¯è‡ªå¸¦é˜²æ­¢é€’å½’çš„åŠŸèƒ½?æ¡†æ¶æœç„¶æ˜¯å¥½ä¸œè¥¿,æ€¥äººä¹‹æ‰€æ€¥å•Š

### ä¼ é€’å‚æ•°

```py
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes_tags"

    def start_requests(self):
        url = 'http://quotes.toscrape.com/'
        tag = getattr(self, 'tag', None)
        if tag is not None:
            url = url + 'tag/' + tag
        yield scrapy.Request(url, self.parse)

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('small.author::text').extract_first(),
            }

        next_page = response.css('li.next a::attr(href)').extract_first()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, self.parse)
```

```sh
 scrapy crawl quotes_tags -o quotes_tags.json -a tag=humor
```

æœ€ç»ˆè·å–çš„ç»“æœåªæœ‰humorç±»å‹çš„

>You can learn more about [handling spider arguments here][spider-arguments].

ä»Šå¤©å°±åˆ°è¿™é‡Œ,æ¶ˆåŒ–ä¸€ä¸‹.

[visualenvå®˜ç½‘]: https://virtualenv.pypa.io "visualenvå®˜ç½‘"
[Scrapyå®˜ç½‘]: https://scrapy.org/ "Scrapyå®˜ç½‘"
[Scrapyå®˜ç½‘Tutorial]: https://docs.scrapy.org/en/latest/intro/tutorial.html "Scrapyå®˜ç½‘Tutorial"
[spider-arguments]: https://docs.scrapy.org/en/latest/topics/spiders.html#spiderargs "spider-arguments"
